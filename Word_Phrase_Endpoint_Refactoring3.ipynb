{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Phrase_Endpoint_Refactoring3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LilySu/BetterBusinessByReview/blob/master/Word_Phrase_Endpoint_Refactoring3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKzD21T4acWg",
        "colab_type": "code",
        "outputId": "83b1117c-60f4-4175-da0e-0bd2516eae72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "import json\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lxml import html\n",
        "from requests import Session\n",
        "from concurrent.futures import ThreadPoolExecutor as Executor\n",
        "import requests\n",
        "import re\n",
        "pd.options.display.max_rows = 999\n",
        "pd.options.display.max_columns = 999\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "base_url = \"https://www.yelp.com/biz/\" \n",
        "api_url = \"/review_feed?sort_by=date_desc&start=\"\n",
        "bid = 'JqQdugxFSmGw797n-qxsMw'\n",
        "\n",
        "\n",
        "class Scraper():\n",
        "    def __init__(self):\n",
        "        self.data = pd.DataFrame()\n",
        "\n",
        "    def get_data(self, n, bid=bid):\n",
        "        with Session() as s:\n",
        "            with s.get(base_url+bid+api_url+str(n*20)) as resp: #makes an http get request to given url and returns response as json\n",
        "                r = json.loads(resp.content) #converts json response into a dictionary\n",
        "                _html = html.fromstring(r['review_list']) #loads from dictionary\n",
        "\n",
        "                dates = _html.xpath(\"//div[@class='review-content']/descendant::span[@class='rating-qualifier']/text()\")\n",
        "                reviews = [el.text for el in _html.xpath(\"//div[@class='review-content']/p\")]\n",
        "                ratings = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
        "\n",
        "                df = pd.DataFrame([dates, reviews, ratings]).T\n",
        "\n",
        "                self.data = pd.concat([self.data,df])\n",
        "\n",
        "    def scrape(self): #makes it faster\n",
        "        # multithreaded looping\n",
        "        with Executor(max_workers=40) as e:\n",
        "            list(e.map(self.get_data, range(10)))\n",
        "\n",
        "s = Scraper()\n",
        "s.scrape()\n",
        "df = s.data\n",
        "df = df.dropna()\n",
        "\n",
        "df['word_segments_unpacked'] = df[1].apply(lambda x: x[1:-1].split(' '))#turn string comma separated list per word\n",
        "\n",
        "df['word_segments_unpacked'] = df['word_segments_unpacked'].astype(str)\n",
        "df['word_segments_unpacked'] = df['word_segments_unpacked'].apply(lambda x: ''.join([str(i) for i in x]))\n",
        "phrase_count = df[['word_segments_unpacked', 2]]\n",
        "\n",
        "\n",
        "s= phrase_count.apply(lambda x: pd.Series(x['word_segments_unpacked']),axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'word_segments_unpacked'\n",
        "\n",
        "phrase_count = phrase_count.drop('word_segments_unpacked', axis=1).join(s)\n",
        "phrase_count = pd.DataFrame(df['word_segments_unpacked'].str.split(',').tolist(), index=df[2]).stack()\n",
        "\n",
        "phrase_count = phrase_count.reset_index()[[0, 2]] # var1 variable is currently labeled 0\n",
        "phrase_count.columns = ['word_segments_unpacked', 'ratings'] # renaming var1\n",
        "phrase_count = phrase_count.reset_index(drop=False)\n",
        "replace_dict_phrase_count = {'[':'',']':'','-':'','!':'','.':'',' ':'',\"'\":''}\n",
        "for key in replace_dict_phrase_count.keys():\n",
        "  phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.replace(key, replace_dict_phrase_count[key])\n",
        "phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.lower()\n",
        "\n",
        "stopwords = [')','(','\\(','\\xa0','0','1','2','3','4','5','6','7','8','9','/','$',\"'d\",\"'ll\",\"'m\",'+','maybe','from','first','here','only','put','where','got','sure','definitely','food','yet','our','go','since','really','very','two',\"n't\",'with','if',\"'s\",'which','came','all','me','(',')','makes','make','were','immediately','get','been','ahead','also','that','one','have','see','what','to','we','had','.',\"'re\",'it','or','he','she','we','us','how','went','no','\"','of','has','by','bit','thing','place','so','ok','and','they','none','was','you',\"'ve\",'did','be','and','but','is','as','&','you','has','-',':','and','had','was','him','so','my','did','would','her','him','it','is','by','bit','thing','place','[',']','while','check-in','=','= =','want', 'good','husband', 'want','love','something','your','they','your','cuz','him',\"i've\",'her','told', 'check', 'i\"m', \"it's\",'they', 'this','its','they','this',\"don't\",'the',',', 'it', 'i\"ve', 'i\"m', '!', '1','2','3','4', '5','6','7','8','9','0','/','.']\n",
        "def filter_stopwords(text):\n",
        "  for i in str(text):\n",
        "    if i not in stopwords:\n",
        "      return str(text)\n",
        "\n",
        "#if item in stopwords list partially matches, delete, single letters like 'i' would be deleted from inside individual words if in list\n",
        "phrase_count = phrase_count[~phrase_count['word_segments_unpacked'].isin(stopwords)]\n",
        "#if the following words fully matches, filter out\n",
        "full_match_list = ['i','a','an','am','at','are','in','on','for','','\\xa0\\xa0','\\xa0','\\(']\n",
        "phrase_count = phrase_count[~phrase_count['word_segments_unpacked'].isin(full_match_list)]\n",
        "\n",
        "#pivot table ratings\n",
        "phrase_count_pivot = pd.pivot_table(phrase_count, index='word_segments_unpacked', columns='ratings', aggfunc='count', fill_value=0)\n",
        "phrase_count_pivot.columns = [''.join(col).strip() for col in phrase_count_pivot.columns.values]#flatten index levels part 1\n",
        "phrase_count_pivot = pd.DataFrame(phrase_count_pivot.to_records())#flatten index levels part 2\n",
        "\n",
        "#if there are no _# star reviews, add a column of zeros\n",
        "required_column_names = ['index1.0 star rating', 'index2.0 star rating','index3.0 star rating','index4.0 star rating','index5.0 star rating']\n",
        "for i in required_column_names:\n",
        "  if i not in phrase_count_pivot.columns:\n",
        "    phrase_count_pivot[i] = 0\n",
        "phrase_count_pivot.sample(10)\n",
        "\n",
        "#replace the original count by getting an exaggerated scaled tally of reviews to calculate score\n",
        "phrase_count_pivot['index1.0 star rating'] = phrase_count_pivot['index1.0 star rating']*(-2)\n",
        "phrase_count_pivot['index2.0 star rating'] = phrase_count_pivot['index2.0 star rating']*(-1)\n",
        "phrase_count_pivot['index3.0 star rating'] = phrase_count_pivot['index3.0 star rating']*(-0.1)\n",
        "phrase_count_pivot['index4.0 star rating'] = phrase_count_pivot['index4.0 star rating']*(1)\n",
        "phrase_count_pivot['index5.0 star rating'] = phrase_count_pivot['index5.0 star rating']*(2)\n",
        "\n",
        "#get a total score from the sum of exaggerated scores\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['index1.0 star rating'] + phrase_count_pivot['index2.0 star rating'] + phrase_count_pivot['index3.0 star rating'] + phrase_count_pivot['index4.0 star rating'] + phrase_count_pivot['index5.0 star rating']\n",
        "\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['score'].div(phrase_count_pivot['score'].max(), axis=0)#normalize\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['score'].round(decimals=4)#round to 4 decimal places\n",
        "phrase_count_pivot = phrase_count_pivot.sort_values(by=('score'), ascending=False)\n",
        "phrase_count_pivot.head(2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_segments_unpacked</th>\n",
              "      <th>index1.0 star rating</th>\n",
              "      <th>index5.0 star rating</th>\n",
              "      <th>index2.0 star rating</th>\n",
              "      <th>index3.0 star rating</th>\n",
              "      <th>index4.0 star rating</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>job</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>small</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_segments_unpacked  index1.0 star rating  index5.0 star rating  \\\n",
              "30                    job                     0                     2   \n",
              "49                  small                     0                     2   \n",
              "\n",
              "    index2.0 star rating  index3.0 star rating  index4.0 star rating  score  \n",
              "30                     0                  -0.0                     0    1.0  \n",
              "49                     0                  -0.0                     0    1.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r-14Zh1YQMpR",
        "colab": {}
      },
      "source": [
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace('\\(', '')\n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace('(', '')\n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace(')', '')#without these, errors incurr\n",
        "\n",
        "worst_terms_list = [] \n",
        "top_terms_list = []\n",
        "x,y = phrase_count_pivot.shape#tuple unpacking to get the length of the dataframe\n",
        "for i in reversed(range(x)):\n",
        "  try:\n",
        "    new_df = df[df[1].str.contains(phrase_count_pivot['word_segments_unpacked'].iloc[i])]#if word appears in review, create a dataframe\n",
        "    neg_first_df = new_df.sort_values(by=2, ascending=True)#rank the dataframe with worst reviews first\n",
        "    pos_first_df = new_df.sort_values(by=2, ascending=False)#rank the dataframe with most positive reviews first\n",
        "    if neg_first_df[1].iloc[0] not in worst_terms_list:#get the lowest star rating review\n",
        "      worst_terms_list.append(neg_first_df[1].iloc[0])#prevent duplicates\n",
        "    if pos_first_df[1].iloc[0] not in top_terms_list:\n",
        "      top_terms_list.append(pos_first_df[1].iloc[0])\n",
        "  except IndexError as e:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMc4NAWQavn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_list = []\n",
        "for i in range(-30,0):#take the worst 30 terms\n",
        "  for list_of_words in worst_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try: \n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word: #find word occurrence in original comma separated word list of reviews\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict_string_from_phrases= {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict_string_from_phrases.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:#if there are less than 30 words after stopword filtering, just get the first word and its occurrence in the original review\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict_string_from_phrases= {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict_string_from_phrases.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "negative_df = pd.DataFrame(negative_list)\n",
        "negative_df = negative_df.reset_index(drop=False)\n",
        "negative_df = negative_df.rename(columns={'index':'score', 0 : 'term'})\n",
        "negative_df = negative_df.drop_duplicates(subset='term')\n",
        "x,y = negative_df.shape#tuple unpacking to get the length of the dataframe\n",
        "if x < 10:\n",
        "  for i in range(-40,-30):\n",
        "    for list_of_words in worst_terms_list:\n",
        "      word_list = list_of_words.split(' ')\n",
        "      for word in word_list:\n",
        "        word = word.replace(',','')\n",
        "        word = word.replace('.','')\n",
        "        try:\n",
        "          if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "            try:\n",
        "              index = word_list.index(word)\n",
        "              string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "              replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "              for key in replace_dict.keys():\n",
        "                string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "              negative_list.append(string_from_phrases)\n",
        "            except ValueError as e:\n",
        "              pass\n",
        "        except IndexError as e:\n",
        "          if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "            try:\n",
        "              index = word_list.index(word)\n",
        "              string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "              replace_dict_string_from_phrases= {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "              for key in replace_dict_string_from_phrases.keys():\n",
        "                string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "              negative_list.append(string_from_phrases)\n",
        "            except ValueError as e:\n",
        "              pass\n",
        "negative_df_addon = pd.DataFrame(negative_list)\n",
        "negative_df_addon = negative_df_addon.reset_index(drop=False)\n",
        "negative_df_addon = negative_df_addon.rename(columns={'index':'score', 0 : 'term'})\n",
        "negative_df = pd.concat([negative_df, negative_df_addon])\n",
        "negative_df = negative_df.head(10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl1Aqp7ta0j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_list = []\n",
        "for i in range(0,30):\n",
        "  for list_of_words in top_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try: \n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "  positive_list.append(string_from_phrases)\n",
        "positive_df = pd.DataFrame(positive_list)\n",
        "positive_df = positive_df.reset_index(drop=False)\n",
        "positive_df = positive_df.rename(columns={'index':'score', 0 : 'term'})\n",
        "positive_df = positive_df.drop_duplicates(subset='term')\n",
        "x,y = positive_df.shape#tuple unpacking to get the length of the dataframe\n",
        "for i in range(30,40):\n",
        "  for list_of_words in top_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "positive_df_addon = pd.DataFrame(negative_list)\n",
        "positive_df_addon = positive_df_addon.reset_index(drop=False)\n",
        "positive_df_addon = positive_df_addon.rename(columns={'index':'score', 0 : 'term'})\n",
        "positive_df = pd.concat([positive_df, positive_df_addon])\n",
        "positive_df = positive_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdxKAvi8M-2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "3af84c2a-86a8-4c65-a0bf-08084f309a74"
      },
      "source": [
        "results = {'positive': [{'term': pos_term, 'score': pos_score} for pos_term, pos_score in zip(positive_df['term'], positive_df['score'])], 'negative': [{'term': neg_term, 'score': neg_score} for neg_term, neg_score in zip(negative_df['term'], negative_df['score'])]}\n",
        "results"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'negative': [{'score': 0,\n",
              "   'term': 'Can I give them zero stars? So I reached out to Israel (owner) during the st week of October for a move on November st'},\n",
              "  {'score': 1,\n",
              "   'term': 'cant move me at the quoterate he had agreed to earlier. Since the moving date was three days away  I agreed to an additional $'},\n",
              "  {'score': 2,\n",
              "   'term': 'them zero stars? So I reached out to Israel (owner) during the st week of October for a move on November st (yep today) Was'},\n",
              "  {'score': 3,\n",
              "   'term': 'agreed to earlier. Since the moving date was three days away  I agreed to an additional $ an hour quote. '},\n",
              "  {'score': 4,\n",
              "   'term': 'to earlier. Since the moving date was three days away  I agreed to an additional $ an hour quote. '},\n",
              "  {'score': 5,\n",
              "   'term': 'today) Was asked to pay $ deposit to confirm the move which I did right away. I called Israel again on Oct th to reconfirm.'},\n",
              "  {'score': 6,\n",
              "   'term': 'called Israel again on Oct th to reconfirm. He told me that he cant move me at the quoterate he had agreed to earlier. Since'},\n",
              "  {'score': 7, 'term': 'away  I agreed to an additional $ an hour quote. '},\n",
              "  {'score': 8,\n",
              "   'term': 'right away. I called Israel again on Oct th to reconfirm. He told me that he cant move me at the quoterate he had agreed'},\n",
              "  {'score': 9,\n",
              "   'term': 'November st (yep today) Was asked to pay $ deposit to confirm the move which I did right away. I called Israel again on Oct'}],\n",
              " 'positive': [{'score': 0,\n",
              "   'term': 'have any minimums  and no job is too small. I only had a bed and a sofa to move  I got a great price  and'},\n",
              "  {'score': 2,\n",
              "   'term': 'These guys do not have any minimums  and no job is too small. I only had a bed and a sofa to move '},\n",
              "  {'score': 6,\n",
              "   'term': 'These guys do not have any minimums  and no job is too small. I only had a bed and a sofa'},\n",
              "  {'score': 7,\n",
              "   'term': 'to move  I got a great price  and they were no nice. Oh yeah  they also spoke English. Thanks Muver'},\n",
              "  {'score': 10,\n",
              "   'term': 'These guys do not have any minimums  and no job is too small. I only had a bed and a sofa to'},\n",
              "  {'score': 12,\n",
              "   'term': 'small. I only had a bed and a sofa to move  I got a great price  and they were no nice. Oh yeah  they also'},\n",
              "  {'score': 14,\n",
              "   'term': 'had a bed and a sofa to move  I got a great price  and they were no nice. Oh yeah  they also spoke English. Thanks'},\n",
              "  {'score': 16,\n",
              "   'term': 'These guys do not have any minimums  and no job is too small. I only had a bed and a sofa to move  I got'},\n",
              "  {'score': 17,\n",
              "   'term': 'nice. Oh yeah  they also spoke English. Thanks Muver'},\n",
              "  {'score': 18,\n",
              "   'term': 'minimums  and no job is too small. I only had a bed and a sofa to move  I got a great price  and they were'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN4eR2M5YOa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}