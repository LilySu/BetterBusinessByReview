{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Phrase_Endpoint_Refactoring3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LilySu/BetterBusinessByReview/blob/master/Word_Phrase_Endpoint_Refactoring3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8mRpZ-WaWPP",
        "colab_type": "code",
        "outputId": "5d21f6f2-006b-485b-a220-cd6d40b9a18e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install scattertext\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scattertext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/6a/d2b2af772934a946cbebb47cb068b4631ed437a264d9cfc7ef5761f95e00/scattertext-0.0.2.56-py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scattertext) (1.3.3)\n",
            "Collecting mock\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from scattertext) (1.12.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (from scattertext) (0.10.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from scattertext) (0.21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scattertext) (1.17.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from scattertext) (0.25.3)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels->scattertext) (0.5.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->scattertext) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->scattertext) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->scattertext) (2018.9)\n",
            "Installing collected packages: mock, scattertext\n",
            "Successfully installed mock-3.0.5 scattertext-0.0.2.56\n",
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/4c/0e0fa8b1e193c1e09a6b72807ff4ca17c78f68f0c0f4459bc8043c66d649/catalogue-0.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (42.0.2)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6b/e07fad36913879757c90ba03d6fb7f406f7279e11dcefc105ee562de63ea/preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.2)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.28.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (8.0.2)\n",
            "Installing collected packages: catalogue, preshed, blis, thinc, spacy\n",
            "  Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Found existing installation: blis 0.2.4\n",
            "    Uninstalling blis-0.2.4:\n",
            "      Successfully uninstalled blis-0.2.4\n",
            "  Found existing installation: thinc 7.0.8\n",
            "    Uninstalling thinc-7.0.8:\n",
            "      Successfully uninstalled thinc-7.0.8\n",
            "  Found existing installation: spacy 2.1.9\n",
            "    Uninstalling spacy-2.1.9:\n",
            "      Successfully uninstalled spacy-2.1.9\n",
            "Successfully installed blis-0.4.1 catalogue-0.2.0 preshed-3.0.2 spacy-2.2.3 thinc-7.3.1\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.17.4)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.2.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (42.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.2.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (8.0.2)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-cp36-none-any.whl size=12011741 sha256=160e81703fca82d278c5d1ff5d2563211a70eb61466b9f3a1a7d2b3b91d8f55f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pnkn2ziw/wheels/6a/47/fb/6b5a0b8906d8e8779246c67d4658fd8a544d4a03a75520197a\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.1.0\n",
            "    Uninstalling en-core-web-sm-2.1.0:\n",
            "      Successfully uninstalled en-core-web-sm-2.1.0\n",
            "Successfully installed en-core-web-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKzD21T4acWg",
        "colab_type": "code",
        "outputId": "7fd5cb0a-63ff-4bca-ecd7-41e48552f07d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "import json\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import spacy\n",
        "# import scattertext as st\n",
        "from lxml import html\n",
        "from requests import Session\n",
        "from concurrent.futures import ThreadPoolExecutor as Executor\n",
        "import requests\n",
        "import re\n",
        "pd.options.display.max_rows = 999\n",
        "pd.options.display.max_columns = 999\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "# nlp = spacy.load(\"en_core_web_sm\")#if you run into problems here, 'Restart Runtime' and run all, it might fix things.\n",
        "base_url = \"https://www.yelp.com/biz/\" \n",
        "api_url = \"/review_feed?sort_by=date_desc&start=\"\n",
        "bid = 'JqQdugxFSmGw797n-qxsMw'\n",
        "# bid = 'Rc1lxc5lSKJYd162JHNMfQ'\n",
        "\n",
        "class Scraper():\n",
        "    def __init__(self):\n",
        "        self.data = pd.DataFrame()\n",
        "\n",
        "    def get_data(self, n, bid=bid):\n",
        "        with Session() as s:\n",
        "            with s.get(base_url+bid+api_url+str(n*20)) as resp: #makes an http get request to given url and returns response as json\n",
        "                r = json.loads(resp.content) #converts json response into a dictionary\n",
        "                _html = html.fromstring(r['review_list']) #loads from dictionary\n",
        "\n",
        "                dates = _html.xpath(\"//div[@class='review-content']/descendant::span[@class='rating-qualifier']/text()\")\n",
        "                reviews = [el.text for el in _html.xpath(\"//div[@class='review-content']/p\")]\n",
        "                ratings = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
        "\n",
        "                df = pd.DataFrame([dates, reviews, ratings]).T\n",
        "\n",
        "                self.data = pd.concat([self.data,df])\n",
        "\n",
        "    def scrape(self): #makes it faster\n",
        "        # multithreaded looping\n",
        "        with Executor(max_workers=40) as e:\n",
        "            list(e.map(self.get_data, range(10)))\n",
        "\n",
        "s = Scraper()\n",
        "s.scrape()\n",
        "df = s.data\n",
        "df = df.dropna()\n",
        "\n",
        "df['word_segments_unpacked'] = df[1].apply(lambda x: x[1:-1].split(' '))\n",
        "\n",
        "df['word_segments_unpacked'] = df['word_segments_unpacked'].astype(str)\n",
        "df['word_segments_unpacked'] = df['word_segments_unpacked'].apply(lambda x: ''.join([str(i) for i in x]))\n",
        "phrase_count = df[['word_segments_unpacked', 2]]\n",
        "\n",
        "\n",
        "s= phrase_count.apply(lambda x: pd.Series(x['word_segments_unpacked']),axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'word_segments_unpacked'\n",
        "\n",
        "phrase_count = phrase_count.drop('word_segments_unpacked', axis=1).join(s)\n",
        "phrase_count = pd.DataFrame(df['word_segments_unpacked'].str.split(',').tolist(), index=df[2]).stack()\n",
        "\n",
        "phrase_count = phrase_count.reset_index()[[0, 2]] # var1 variable is currently labeled 0\n",
        "phrase_count.columns = ['word_segments_unpacked', 'ratings'] # renaming var1\n",
        "phrase_count = phrase_count.reset_index(drop=False)\n",
        "replace_dict_phrase_count = {'[':'',']':'','-':'','!':'','.':'',' ':'',\"'\":''}\n",
        "for key in replace_dict_phrase_count.keys():\n",
        "  phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.replace(key, replace_dict_phrase_count[key])\n",
        "phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.lower()\n",
        "\n",
        "stopwords = ['\\xa0','0','1','2','3','4','5','6','7','8','9','/','$',\"'d\",\"'ll\",\"'m\",'+','maybe','from','first','here','only','put','where','got','sure','definitely','food','yet','our','go','since','really','very','two',\"n't\",'with','if',\"'s\",'which','came','all','me','(',')','makes','make','were','immediately','get','been','ahead','also','that','one','have','see','what','to','we','had','.',\"'re\",'it','or','he','she','we','us','how','went','no','\"','of','has','by','bit','thing','place','so','ok','and','they','none','was','you',\"'ve\",'did','be','and','but','is','as','&','you','has','-',':','and','had','was','him','so','my','did','would','her','him','it','is','by','bit','thing','place','[',']','while','check-in','=','= =','want', 'good','husband', 'want','love','something','your','they','your','cuz','him',\"i've\",'her','told', 'check', 'i\"m', \"it's\",'they', 'this','its','they','this',\"don't\",'the',',', 'it', 'i\"ve', 'i\"m', '!', '1','2','3','4', '5','6','7','8','9','0','/','.']\n",
        "def filter_stopwords(text):\n",
        "  for i in str(text):\n",
        "    if i not in stopwords:\n",
        "      return str(text)\n",
        "\n",
        "phrase_count = phrase_count[~phrase_count['word_segments_unpacked'].isin(stopwords)]#if item in stopwords list partially matches, delete\n",
        "\n",
        "#full matches\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='i']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='a']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='an']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='am']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='at']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='are']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='in']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='on']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='for']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='\\xa0\\xa0']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='\\xa0']\n",
        "phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='\\(']\n",
        "\n",
        "phrase_count_pivot = pd.pivot_table(phrase_count, index='word_segments_unpacked', columns='ratings', aggfunc='count', fill_value=0)\n",
        "phrase_count_pivot.columns = [''.join(col).strip() for col in phrase_count_pivot.columns.values]#flatten index levels part 1\n",
        "phrase_count_pivot = pd.DataFrame(phrase_count_pivot.to_records())#flatten index levels part 2\n",
        "\n",
        "required_column_names = ['index1.0 star rating', 'index2.0 star rating','index3.0 star rating','index4.0 star rating','index5.0 star rating']\n",
        "\n",
        "for i in required_column_names:\n",
        "  if i not in phrase_count_pivot.columns:\n",
        "    phrase_count_pivot[i] = 0\n",
        "phrase_count_pivot.sample(10)\n",
        "\n",
        "# #scale\n",
        "phrase_count_pivot['index1.0 star rating'] = phrase_count_pivot['index1.0 star rating']*(-2)\n",
        "phrase_count_pivot['index2.0 star rating'] = phrase_count_pivot['index2.0 star rating']*(-1)\n",
        "phrase_count_pivot['index3.0 star rating'] = phrase_count_pivot['index3.0 star rating']*(-0.1)\n",
        "phrase_count_pivot['index4.0 star rating'] = phrase_count_pivot['index4.0 star rating']*(1)\n",
        "phrase_count_pivot['index5.0 star rating'] = phrase_count_pivot['index5.0 star rating']*(2)\n",
        "\n",
        "\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['index1.0 star rating'] + phrase_count_pivot['index2.0 star rating'] + phrase_count_pivot['index3.0 star rating'] + phrase_count_pivot['index4.0 star rating'] + phrase_count_pivot['index5.0 star rating']\n",
        "\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['score'].div(phrase_count_pivot['score'].max(), axis=0)\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['score'].round(decimals=4)\n",
        "phrase_count_pivot = phrase_count_pivot.sort_values(by=('score'), ascending=False)\n",
        "phrase_count_pivot.head(2)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_segments_unpacked</th>\n",
              "      <th>index1.0 star rating</th>\n",
              "      <th>index5.0 star rating</th>\n",
              "      <th>index2.0 star rating</th>\n",
              "      <th>index3.0 star rating</th>\n",
              "      <th>index4.0 star rating</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>job</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>small</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_segments_unpacked  index1.0 star rating  index5.0 star rating  \\\n",
              "30                    job                     0                     2   \n",
              "49                  small                     0                     2   \n",
              "\n",
              "    index2.0 star rating  index3.0 star rating  index4.0 star rating  score  \n",
              "30                     0                  -0.0                     0    1.0  \n",
              "49                     0                  -0.0                     0    1.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r-14Zh1YQMpR",
        "colab": {}
      },
      "source": [
        "import math \n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace('\\(', '')\n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace('(', '')\n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace(')', '')\n",
        "worst_terms_list = [] \n",
        "top_terms_list = []\n",
        "x,y = phrase_count_pivot.shape#tuple unpacking to get the length of the dataframe\n",
        "# for i in reversed(range(most_neg)):#get a fraction of the total instances\n",
        "for i in reversed(range(x)):\n",
        "  try:\n",
        "    new_df = df[df[1].str.contains(phrase_count_pivot['word_segments_unpacked'].iloc[i])]#if word appears in review, create a dataframe\n",
        "    neg_first_df = new_df.sort_values(by=2, ascending=True)#rank the dataframe with worst reviews first\n",
        "    pos_first_df = new_df.sort_values(by=2, ascending=False)#rank the dataframe with most positive reviews first\n",
        "    if neg_first_df[1].iloc[0] not in worst_terms_list:#get the lowest star rating review\n",
        "      worst_terms_list.append(neg_first_df[1].iloc[0])#prevent duplicates\n",
        "    if pos_first_df[1].iloc[0] not in top_terms_list:\n",
        "      top_terms_list.append(pos_first_df[1].iloc[0])\n",
        "  except IndexError as e:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMc4NAWQavn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_list = []\n",
        "# for i in reversed(range(len(worst_terms_list))):\n",
        "for i in range(-30,0):\n",
        "  for list_of_words in worst_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try: \n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict_string_from_phrases= {' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict_string_from_phrases.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict_string_from_phrases= {' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict_string_from_phrases.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "negative_df = pd.DataFrame(negative_list)\n",
        "negative_df = negative_df.reset_index(drop=False)\n",
        "negative_df = negative_df.rename(columns={'index':'score', 0 : 'term'})\n",
        "negative_df = negative_df.drop_duplicates(subset='term')\n",
        "x,y = negative_df.shape#tuple unpacking to get the length of the dataframe\n",
        "if x < 10:\n",
        "  for i in range(-40,-30):\n",
        "    for list_of_words in worst_terms_list:\n",
        "      word_list = list_of_words.split(' ')\n",
        "      for word in word_list:\n",
        "        word = word.replace(',','')\n",
        "        word = word.replace('.','')\n",
        "        try:\n",
        "          if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "            try:\n",
        "              index = word_list.index(word)\n",
        "              string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "              replace_dict = {' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "              for key in replace_dict.keys():\n",
        "                string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "              negative_list.append(string_from_phrases)\n",
        "            except ValueError as e:\n",
        "              pass\n",
        "        except IndexError as e:\n",
        "          if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "            try:\n",
        "              index = word_list.index(word)\n",
        "              string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "              replace_dict_string_from_phrases= {' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "              for key in replace_dict_string_from_phrases.keys():\n",
        "                string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "              negative_list.append(string_from_phrases)\n",
        "            except ValueError as e:\n",
        "              pass\n",
        "negative_df_addon = pd.DataFrame(negative_list)\n",
        "negative_df_addon = negative_df_addon.reset_index(drop=False)\n",
        "negative_df_addon = negative_df_addon.rename(columns={'index':'score', 0 : 'term'})\n",
        "negative_df = pd.concat([negative_df, negative_df_addon])\n",
        "negative_df = negative_df.head(10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl1Aqp7ta0j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_list = []\n",
        "for i in range(0,30):\n",
        "  for list_of_words in top_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try: \n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "  positive_list.append(string_from_phrases)\n",
        "positive_df = pd.DataFrame(positive_list)\n",
        "positive_df = positive_df.reset_index(drop=False)\n",
        "positive_df = positive_df.rename(columns={'index':'score', 0 : 'term'})\n",
        "positive_df = positive_df.drop_duplicates(subset='term')\n",
        "x,y = positive_df.shape#tuple unpacking to get the length of the dataframe\n",
        "for i in range(30,40):\n",
        "  for list_of_words in top_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "positive_df_addon = pd.DataFrame(negative_list)\n",
        "positive_df_addon = positive_df_addon.reset_index(drop=False)\n",
        "positive_df_addon = positive_df_addon.rename(columns={'index':'score', 0 : 'term'})\n",
        "positive_df = pd.concat([positive_df, positive_df_addon])\n",
        "positive_df = positive_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdxKAvi8M-2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "d2c735a1-ac4d-4ccb-fcfe-801af79aabea"
      },
      "source": [
        "results = {'positive': [{'term': pos_term, 'score': pos_score} for pos_term, pos_score in zip(positive_df['term'], positive_df['score'])], 'negative': [{'term': neg_term, 'score': neg_score} for neg_term, neg_score in zip(negative_df['term'], negative_df['score'])]}\n",
        "results"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'negative': [{'score': 0,\n",
              "   'term': 'Can I give them zero stars? So I reached out to Israel (owner) during the st week of October for a move on November st'},\n",
              "  {'score': 1,\n",
              "   'term': 'cant move me at the quoterate he had agreed to earlier. Since the moving date was three days away  I agreed to an additional $'},\n",
              "  {'score': 2,\n",
              "   'term': 'them zero stars? So I reached out to Israel (owner) during the st week of October for a move on November st (yep today) Was'},\n",
              "  {'score': 3,\n",
              "   'term': 'agreed to earlier. Since the moving date was three days away  I agreed to an additional $ an hour quote. '},\n",
              "  {'score': 4,\n",
              "   'term': 'to earlier. Since the moving date was three days away  I agreed to an additional $ an hour quote. '},\n",
              "  {'score': 5,\n",
              "   'term': 'today) Was asked to pay $ deposit to confirm the move which I did right away. I called Israel again on Oct th to reconfirm.'},\n",
              "  {'score': 6,\n",
              "   'term': 'called Israel again on Oct th to reconfirm. He told me that he cant move me at the quoterate he had agreed to earlier. Since'},\n",
              "  {'score': 7, 'term': 'away  I agreed to an additional $ an hour quote. '},\n",
              "  {'score': 8,\n",
              "   'term': 'right away. I called Israel again on Oct th to reconfirm. He told me that he cant move me at the quoterate he had agreed'},\n",
              "  {'score': 9,\n",
              "   'term': 'November st (yep today) Was asked to pay $ deposit to confirm the move which I did right away. I called Israel again on Oct'}],\n",
              " 'positive': [{'score': 0,\n",
              "   'term': 'have any minimums  and no job is too small. I only had a bed and a sofa to move  I got a great price  and'},\n",
              "  {'score': 2,\n",
              "   'term': 'These guys do not have any minimums  and no job is too small. I only had a bed and a sofa to move '},\n",
              "  {'score': 6,\n",
              "   'term': 'These guys do not have any minimums  and no job is too small. I only had a bed and a sofa'},\n",
              "  {'score': 7,\n",
              "   'term': 'to move  I got a great price  and they were no nice. Oh yeah  they also spoke English. \\xa0Thanks Muver'},\n",
              "  {'score': 10,\n",
              "   'term': 'These guys do not have any minimums  and no job is too small. I only had a bed and a sofa to'},\n",
              "  {'score': 12,\n",
              "   'term': 'small. I only had a bed and a sofa to move  I got a great price  and they were no nice. Oh yeah  they also'},\n",
              "  {'score': 14,\n",
              "   'term': 'had a bed and a sofa to move  I got a great price  and they were no nice. Oh yeah  they also spoke English. \\xa0Thanks'},\n",
              "  {'score': 16,\n",
              "   'term': 'These guys do not have any minimums  and no job is too small. I only had a bed and a sofa to move  I got'},\n",
              "  {'score': 17,\n",
              "   'term': 'nice. Oh yeah  they also spoke English. \\xa0Thanks Muver'},\n",
              "  {'score': 18,\n",
              "   'term': 'minimums  and no job is too small. I only had a bed and a sofa to move  I got a great price  and they were'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN4eR2M5YOa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}