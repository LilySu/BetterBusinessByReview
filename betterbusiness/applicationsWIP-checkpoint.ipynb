{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "# import wget\n",
    "# from elevate import elevate\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import _pickle as pickle\n",
    "from flask import Flask\n",
    "# from flask_cors import CORS\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import scattertext as st\n",
    "from pprint import pprint\n",
    "\n",
    "#Imports\n",
    "from json import loads\n",
    "from lxml import html\n",
    "from requests import Session\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor as Executor\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('C:\\\\Users\\\\lilyx\\\\BR-Data-Science\\\\test-version-v25\\\\trimmed_20k.csv')\n",
    "\n",
    "lg_url = \"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz\"\n",
    "\n",
    "r = requests.get(lg_url, allow_redirects=True)\n",
    "open('sm.zip', 'wb').write(r.content)\n",
    "\n",
    "tar = tarfile.open('sm.zip', \"r:gz\")\n",
    "tar.extractall('down_sm')\n",
    "\n",
    "nlp = spacy.load(\"./down_sm/en_core_web_sm-2.1.0/en_core_web_sm/en_core_web_sm-2.1.0\")\n",
    "nlp.Defaults.stop_words |= {'check-in','=','= =','male','u','want', 'u want', 'cuz','him',\"i've\", 'deaf','on', 'her','told','told him','ins', '1 check','I', 'i\"m', 'i', ' ', 'it', \"it's\", 'it.','they','coffee','place','they', 'the', 'this','its', 'l','-','they','this','don\"t','the ', ' the', 'it', 'i\"ve', 'i\"m', '!', '1','2','3','4', '5','6','7','8','9','0','/','.',','}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_termfreq_from_url(yelp_url, from_isbn=False):\n",
    "    '''Takes a url, scrape site for reviews\n",
    "    and calculates the term frequencies \n",
    "    sorts and returns the top 10 as a json object\n",
    "    containing term, highratingscore, poorratingscore.'''\n",
    "    \n",
    "    base_url = \"https://www.yelp.com/biz/\" # add business id\n",
    "    api_url = \"/review_feed?sort_by=date_desc&start=\"\n",
    "    bid = url[24:] # business id\n",
    "\n",
    "    class Scraper():\n",
    "        def __init__(self):\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "        def get_data(self, n, bid=bid):\n",
    "            with Session() as s:\n",
    "                with s.get(base_url+bid+api_url+str(n*20)) as resp: #makes an http get request to given url and returns response as json\n",
    "                    r = loads(resp.content) #converts json response into a dictionary\n",
    "                    _html = html.fromstring(r['review_list']) #loads from dictionary\n",
    "\n",
    "                    dates = _html.xpath(\"//div[@class='review-content']/descendant::span[@class='rating-qualifier']/text()\")\n",
    "                    reviews = [el.text for el in _html.xpath(\"//div[@class='review-content']/p\")]\n",
    "                    ratings = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
    "\n",
    "                    df = pd.DataFrame([dates, reviews, ratings]).T\n",
    "\n",
    "                    self.data = pd.concat([self.data,df])\n",
    "\n",
    "        def scrape(self): #makes it faster\n",
    "            # multithreaded looping\n",
    "            with Executor(max_workers=40) as e:\n",
    "                list(e.map(self.get_data, range(10)))\n",
    "\n",
    "    s = Scraper()\n",
    "    s.scrape()\n",
    "    df = s.data\n",
    "    df = df.sample(100)\n",
    "\n",
    "    corpus = st.CorpusFromPandas(df, \n",
    "                             category_col=2, \n",
    "                             text_col=1,\n",
    "                             nlp=nlp).build()\n",
    "\n",
    "    term_freq_df = corpus.get_term_freq_df()\n",
    "    term_freq_df['highratingscore'] = corpus.get_scaled_f_scores('5.0 star rating')\n",
    "\n",
    "    term_freq_df['poorratingscore'] = corpus.get_scaled_f_scores('1.0 star rating')\n",
    "\n",
    "    df = term_freq_df.sort_values(by= 'poorratingscore', ascending = False)\n",
    "\n",
    "    df['highratingscore'] = round(df['highratingscore'], 2)\n",
    "    df['poorratingscore'] = round(df['poorratingscore'], 2)\n",
    "    \n",
    "    df.to_json(r'test.json', orient='table')\n",
    "#     #scrape\n",
    "    \n",
    "#     #instantiate empty list of rating scores:\n",
    "#     ratings = []\n",
    "    \n",
    "#     #iterate over the doc object for each book in the df to get the similarity score and append to list\n",
    "#     for doc in df.docs:\n",
    "#         sim = eachterm.generateRatingScores(doc)#returns scores\n",
    "#         ratings.append(sim)\n",
    "        \n",
    "#     #sort the list and grab the top 10:\n",
    "#     if from_isbn:\n",
    "#         #skip the 0th ranked book which will be the bookused to get the yelp_url of the description:\n",
    "#         top10 = pd.Series(sims).sort_values(ascending=False).iloc[1:11]\n",
    "#     else:\n",
    "#         top10 = pd.Series(sims).sort_values(ascending=False).iloc[:10]\n",
    "\n",
    "#     #instantiate empty list to store the python dicts of each book\n",
    "#     term = []\n",
    "\n",
    "#     #iterate thru the top 10 ranked simlilar books and populate the book list w/ dictionaries for each book\n",
    "\n",
    "#     for i in top10.index:\n",
    "#         term = {}\n",
    "#     #     term['word'] = df.iloc[i]['word']\n",
    "#         term['highratingscore'] = df.iloc[i]['highratingscore']\n",
    "#         term['poorratingscore'] = str(df.iloc[i]['poorratingscore'])\n",
    "#         term.append(term)\n",
    "#     return json.dumps(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_url = 'https://www.yelp.com/biz/lobster-house-all-you-can-eat-seafood-rego-park-30'\n",
    "get_termfreq_from_url(yelp_url, from_isbn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(get_termfreq_from_url, open('termFr01.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7de79c234168>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    895\u001b[0m                     \u001b[1;31m# used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 897\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "data = json.dumps({'yelp_url': 'https://www.yelp.com/biz/lobster-house-all-you-can-eat-seafood-rego-park-30'})\n",
    "\n",
    "url = 'http://localhost:9000/api'\n",
    "\n",
    "send = requests.post(url, data)\n",
    "\n",
    "print(send.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "import wget\n",
    "\n",
    "import _pickle as pickle\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "import scattertext as st\n",
    "from pprint import pprint\n",
    "\n",
    "#Imports\n",
    "from json import loads\n",
    "from lxml import html\n",
    "from requests import Session\n",
    "import pandas as pd\n",
    "from itertools import count\n",
    "import os\n",
    "from flask import Flask, render_template, request\n",
    "from flask_cors import CORS\n",
    "\n",
    "#we are not using jupyter notebook, so no jsonify\n",
    "\n",
    "#pred logic, grabs information for us from post\n",
    "def ValuePredictor(yelp_url):\n",
    "    '''Takes a url, scrape site for reviews\n",
    "    and calculates the term frequencies \n",
    "    sorts and returns the top 10 as a json object\n",
    "    containing term, highratingscore, poorratingscore.'''\n",
    "    \n",
    "    base_url = \"https://www.yelp.com/biz/\" # add business id\n",
    "    api_url = \"/review_feed?sort_by=date_desc&start=\"\n",
    "    bid = yelp_url.replace('https://www.yelp.com/biz/','')\n",
    "\n",
    "    class Scraper():\n",
    "        def __init__(self):\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "        def get_data(self, n, bid=bid):\n",
    "            with Session() as s:\n",
    "                with s.get(base_url+bid+api_url+str(n*20)) as resp: #makes an http get request to given url and returns response as json\n",
    "                    r = resp.json() #converts json response into a dictionary\n",
    "                    _html = html.fromstring(r['review_list']) #loads from dictionary\n",
    "\n",
    "                    dates = _html.xpath(\"//div[@class='review-content']/descendant::span[@class='rating-qualifier']/text()\")\n",
    "                    reviews = [el.text for el in _html.xpath(\"//div[@class='review-content']/p\")]\n",
    "                    ratings = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
    "\n",
    "                    df = pd.DataFrame([dates, reviews, ratings]).T\n",
    "\n",
    "                    self.data = pd.concat([self.data,df])\n",
    "\n",
    "        def scrape(self): #makes it faster\n",
    "            # multithreaded looping\n",
    "            with Executor(max_workers=40) as e:\n",
    "                list(e.map(self.get_data, range(10)))\n",
    "\n",
    "    s = Scraper()\n",
    "    s.scrape()\n",
    "    df = s.data\n",
    "    df = df.sample(100)\n",
    "\n",
    "    corpus = st.CorpusFromPandas(df, \n",
    "                             category_col=2, \n",
    "                             text_col=1,\n",
    "                             nlp=nlp).build()\n",
    "\n",
    "    term_freq_df = corpus.get_term_freq_df()\n",
    "    term_freq_df['highratingscore'] = corpus.get_scaled_f_scores('5.0 star rating')\n",
    "\n",
    "    term_freq_df['poorratingscore'] = corpus.get_scaled_f_scores('1.0 star rating')\n",
    "\n",
    "    df = term_freq_df.sort_values(by= 'poorratingscore', ascending = False)\n",
    "\n",
    "    df['highratingscore'] = round(df['highratingscore'], 2)\n",
    "    df['poorratingscore'] = round(df['poorratingscore'], 2)\n",
    "    \n",
    "    df.to_json(r'test.json', orient='table')#similar to the logic of make_predict function\n",
    "#all the above does the prediction for you\n",
    "\n",
    "#app\n",
    "app=Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "#routes\n",
    "@app.route('/')#defaults to this just in case, legacy reasons\n",
    "@app.route('/index')\n",
    "def index():\n",
    "    return flask.render_template('index.html')#we are going to have a form\n",
    "\n",
    "#we have to have something to hold and run the results page\n",
    "@app.route('/result', methods = ['POST'])\n",
    "def result():#will capture our predictions, handles result\n",
    "#result will grab post from index.html, instead of host\n",
    "    if request.method == 'POST':\n",
    "        content_type = request.headers[\"content-type\"]\n",
    "        if content_type == \"application/json\":\n",
    "        #take information from form with nominal data, let html do the work convert to dictionary\n",
    "            to_predict_list = request.form.to_dict()\n",
    "            to_predict_list = list(to_predict_list.values())\n",
    "            to_predict_list = list(map(str, to_predict_list))\n",
    "            result = ValuePredictor(to_predict_list)\n",
    "        return render_template(\"results.html\", prediction=result)\n",
    "    else:\n",
    "        return jsonify(\"Content-Type is not application/json\")\n",
    "#app run, so we don't export as an actual app\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=9000, debug=True)\n",
    "#windows go to system variables to shut down."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
